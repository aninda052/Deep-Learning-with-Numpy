{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnArchitecture = [\n",
    "    {'inputDim':2,'outputDim':25,'activation':'relu'},\n",
    "    {'inputDim':25,'outputDim':50,'activation':'relu'},\n",
    "    {'inputDim':50,'outputDim':50,'activation':'relu'},\n",
    "    {'inputDim':50,'outputDim':25,'activation':'relu'},\n",
    "    {'inputDim':25,'outputDim':1,'activation':'sigmoid'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeLayer (nnArchitecture,seed=5) :\n",
    "    np.random.seed(seed)\n",
    "    paramsValues = {}\n",
    "    \n",
    "    for idx,layerInfo in enumerate(nnArchitecture):\n",
    "        layerNum = idx+1\n",
    "        inputDim = layerInfo['inputDim']\n",
    "        outputDim = layerInfo['outputDim']\n",
    "        \n",
    "        paramsValues ['W_' + str(layerNum)] = np.random.randn(outputDim,inputDim)*.1\n",
    "        paramsValues ['B_' + str(layerNum)] = np.random.randn(outputDim,1)*.1\n",
    "        \n",
    "    return paramsValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoidBackward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def reluBackward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleLayerForwordPropagation (weightMatrixOfCurrentLayer,biasOfCurrentLayer,outputValueOfPreviousLayer,activation = 'relu' ) :\n",
    "    \n",
    "    affineTransformationOfCurrentLayer = np.dot(weightMatrixOfCurrentLayer,\n",
    "                                    outputValueOfPreviousLayer) + biasOfCurrentLayer\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(affineTransformationOfCurrentLayer), affineTransformationOfCurrentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullForwordPropagation(inputLayer,nnArchitecture,layerWeightBias) :\n",
    "    \n",
    "    outputOfCurrentLayer = inputLayer # input layer\n",
    "    forwordMemory = {}\n",
    "    \n",
    "    for layerIdx,layerInfo in enumerate(nnArchitecture):\n",
    "        \n",
    "        currentLayerNum = layerIdx+1\n",
    "\n",
    "        currentLayerWeight = layerWeightBias['W_'+str(currentLayerNum)]\n",
    "        currentLayerBias = layerWeightBias['B_'+str(currentLayerNum)]\n",
    "        currentLayerActivation = layerInfo['activation']\n",
    "        inputOfCurrentLayer = outputOfCurrentLayer\n",
    "\n",
    "        outputOfCurrentLayer,affineTransformationOfCurrentLayer = singleLayerForwordPropagation(currentLayerWeight,currentLayerBias,inputOfCurrentLayer,currentLayerActivation)\n",
    "        \n",
    "\n",
    "        forwordMemory['A_'+str(layerIdx)] = inputOfCurrentLayer\n",
    "        forwordMemory['Z_'+str(currentLayerNum)] = affineTransformationOfCurrentLayer\n",
    "        \n",
    "    return outputOfCurrentLayer , forwordMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleLayerBackwordPropagation (weightMatrixOfCurrentLayer,backwordInputValueOfCurrentLayer\n",
    "                                    ,forwordAffineTransformationOfCurrentLayer,forwordOutputOfpreviousLayer,activation = 'relu' ) :\n",
    "    \n",
    "    m = forwordOutputOfpreviousLayer.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    backwordAffineTransformationOfCurrentLayer = activation_func(backwordInputValueOfCurrentLayer,forwordAffineTransformationOfCurrentLayer)\n",
    "\n",
    "    backwordOutputOfCurrentLayer = np.dot(weightMatrixOfCurrentLayer.T,backwordAffineTransformationOfCurrentLayer)\n",
    "    backwordBiasOfCurrentLayer = np.sum(backwordAffineTransformationOfCurrentLayer, axis=1, keepdims=True)/m\n",
    "    backwordWeightOfCurrentLayer = np.dot(backwordAffineTransformationOfCurrentLayer,forwordOutputOfpreviousLayer.T)/m\n",
    "    \n",
    "\n",
    "        \n",
    "    return backwordOutputOfCurrentLayer,backwordWeightOfCurrentLayer,backwordBiasOfCurrentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullBackwordPropagation(forwordOutput,actualValue,nnArchitecture,layerWeightBias,forwordMemory) :\n",
    "    \n",
    "    actualValue = actualValue.reshape(forwordOutput.shape)\n",
    "    \n",
    "\n",
    "    backwordOutputOfCurrentLayer = - (np.divide(actualValue, forwordOutput) - np.divide(1 - actualValue, 1 - forwordOutput))\n",
    "\n",
    "\n",
    "    backwordWeightBias = {}\n",
    "    actualValue = actualValue.reshape(forwordOutput.shape[1])\n",
    "    for previousLayerNum,layerInfo in reversed(list(enumerate(nnArchitecture))):\n",
    "\n",
    "        currentLayerNum = previousLayerNum+1\n",
    "        \n",
    "\n",
    "        \n",
    "        currentLayerWeight = layerWeightBias['W_'+str(currentLayerNum)]\n",
    "        forwordOutputOfpreviousLayer = forwordMemory['A_'+str(previousLayerNum)]\n",
    "\n",
    "        \n",
    "        forwordAffineTransformationOfCurrentLayer = forwordMemory['Z_'+str(currentLayerNum)]\n",
    "        currentLayerActivation = layerInfo['activation']\n",
    "        \n",
    "        backwordInputOfCurrentLayer = backwordOutputOfCurrentLayer\n",
    "        \n",
    "        backwordOutputOfCurrentLayer,backwordWeight,backwordBias = singleLayerBackwordPropagation(currentLayerWeight\n",
    "            ,backwordInputOfCurrentLayer ,forwordAffineTransformationOfCurrentLayer,forwordOutputOfpreviousLayer\n",
    "                                                                ,currentLayerActivation)\n",
    "        \n",
    "\n",
    "        backwordWeightBias['W_'+str(currentLayerNum)] = backwordWeight\n",
    "        backwordWeightBias['B_'+str(currentLayerNum)] = backwordBias\n",
    "        \n",
    "    return   backwordWeightBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerWeightBiasUpdate (layerWeightBias,backwordWeightBias,nnArchitecture,learningrate):\n",
    "    \n",
    "    for currentLayerNum,layerInfo in enumerate(nnArchitecture,1):\n",
    "\n",
    "        layerWeightBias['W_'+str(currentLayerNum)] -= learningrate * backwordWeightBias['W_'+str(currentLayerNum)]\n",
    "        \n",
    "        layerWeightBias['B_'+str(currentLayerNum)] -= learningrate * backwordWeightBias['B_'+str(currentLayerNum)]\n",
    "\n",
    "        \n",
    "    return layerWeightBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
